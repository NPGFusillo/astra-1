#!/usr/bin/env python3
import click

# Common options.
@click.group()
@click.option("-v", "verbose", default=False, is_flag=True, help="verbose mode")
@click.pass_context
def cli(context, verbose):
    context.ensure_object(dict)
    context.obj["verbose"] = verbose
    # Overwrite settings in ~/.astra/astra.yml
    # from astra import log
    # log.set_level(10 if verbose else 20)


@cli.command()
@click.option("--drop-tables", is_flag=True)
@click.option("--delay", default=10)
# default to grant permissions
@click.option("--no-grant-permissions", is_flag=True, default=False)
def initdb(drop_tables, delay, no_grant_permissions):
    """Initialize the database."""
    from time import sleep
    from astra.utils import log
    from astra.models import (
        base,
        apogee,
        apogeenet,
        aspcap,
        #boss,
        #classifier,
        ferre,
        #lineforest,
        #madgics,
        #mdwarftype,
        #slam,
        #snow_white,
        source,
        spectrum,
        #the_payne
    )
    
    models = base.BaseModel.__subclasses__()
    with base.database.atomic():
        if drop_tables:
            log.warning(f"Dropping database tables in {delay} seconds..")
            sleep(delay)
            base.database.drop_tables(models, cascade=True)
        
        base.database.create_tables(models)
    log.info(f"Created {len(models)} database tables: {models}")
    
    if not no_grant_permissions:
        schema = base.BaseModel._meta.schema
        log.info(f"Granting permissions on schema {schema} to role 'sdss'")
        base.database.execute_sql(f"GRANT ALL PRIVILEGES ON SCHEMA {schema} TO GROUP sdss;")
        base.database.execute_sql(f"GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA {schema} to sdss;")

    return None


@cli.command(context_settings=dict(ignore_unknown_options=True))
@click.option("--apred", default=None)
@click.option("--run2d", default=None)
@click.option("--limit", default=None)
@click.option("--include-dr17", is_flag=True, default=False)
def migrate(apred, run2d, limit, include_dr17):
    """Migrate data from the SDSS5 database."""

    from astra.utils import log
    from astra.models.source import Source

    from astra.migrations.apogee import (
        migrate_sdss4_dr17_apogee_spectra_from_sdss5_catalogdb, 
        migrate_apvisit_metadata_from_image_headers,
        fix_version_id_edge_cases
    )
    from astra.migrations.boss import (
        migrate_spectra_from_spall_file,
        migrate_specfull_metadata_from_image_headers
    )
    from astra.migrations.catalog import (
        migrate_gaia_source_ids,
        migrate_healpix,
        migrate_tic_v8_identifier,
        migrate_twomass_photometry,
        migrate_unwise_photometry,
        migrate_glimpse_photometry,
        migrate_gaia_dr3_astrometry_and_photometry,
        migrate_zhang_stellar_parameters,
        migrate_bailer_jones_distances
    )
    from astra.migrations.misc import (
        compute_f_night_time_for_boss_visits, 
        compute_f_night_time_for_apogee_visits,
        set_missing_gaia_source_ids_to_null,
        compute_n_neighborhood,
        update_visit_spectra_counts
    )
    from astra.migrations.reddening import update_reddening
    from astra.migrations.targeting import migrate_carton_assignments_to_bigbitfield

    log.info("Starting ingestion. This will take a long time.")

    if include_dr17:
        log.info(f"Ingesting DR17 APOGEE spectra")
        migrate_sdss4_dr17_apogee_spectra_from_sdss5_catalogdb(limit=limit)

    if run2d is not None:
        log.info(f"Ingesting SDSS5 BOSS spectra with run2d={run2d}")
        migrate_spectra_from_spall_file(run2d, limit=limit)

        log.info(f"Migrating BOSS metadata from headers")
        migrate_specfull_metadata_from_image_headers()
            
    if apred is not None:
        from astra.migrations.apogee import (        
            migrate_1p3_apvisit_from_sdss5_apogee_drpdb as migrate_apvisit_from_sdss5_apogee_drpdb,
            migrate_apstar_from_sdss5_database,
        )

        log.info(f"Ingesting SDSS5 APOGEE spectra with apred={apred}")
        migrate_apvisit_from_sdss5_apogee_drpdb(apred, limit=limit)
        
        fix_version_id_edge_cases()
        migrate_apstar_from_sdss5_database(apred, limit=limit)
        
    log.info(f"Migrating carton assignments")
    migrate_carton_assignments_to_bigbitfield()

    log.info(f"Migrating HEALPix")
    migrate_healpix()

    log.info(f"Migrating Gaia source identifiers")
    migrate_gaia_source_ids()

    log.info(f"Migrating Gaia astrometry and photometry")
    migrate_gaia_dr3_astrometry_and_photometry()
    
    log.info(f"Migrating Zhang et al stellar parameters")
    migrate_zhang_stellar_parameters()

    log.info(f"Migrating 2MASS photometry")
    migrate_twomass_photometry()

    log.info(f"Migrating unWISE photometry")
    migrate_unwise_photometry()

    log.info(f"Migrating GLIMPSE photometry")
    migrate_glimpse_photometry()

    log.info(f"Migrating TIC v8 identifiers")
    migrate_tic_v8_identifier()

    log.info(f"Migrating Bailer-Jones distances")
    migrate_bailer_jones_distances()
    
    log.info(f"Migration from SDSS5 catalogdb complete")
    set_missing_gaia_source_ids_to_null()
    
    log.info(f"Computing f_night_fraction for BOSS visits")
    compute_f_night_time_for_boss_visits()

    log.info(f"Computing f_night_fraction for APOGEE visits")
    compute_f_night_time_for_apogee_visits()
    
    log.info(f"Computing neighbourhood size")
    compute_n_neighborhood()
    
    log.info("Computing visit spectra counts")
    update_visit_spectra_counts()
    
    log.info(f"Computing extinction")
    update_reddening()
    
    log.info(f"Migrating apVisit metadata from image headers")
    migrate_apvisit_metadata_from_image_headers()
    log.info("Done")
        

@cli.command(context_settings=dict(ignore_unknown_options=True))
@click.option("--slurm-profile", default=None, help="Use Slurm profile specified in Astra config file. If None is given, it will default to the profile for the task name, or `default`.")
@click.option("--slurm-dir", default=None)
@click.option("--limit", default=None, type=int)
def create_mwm_products(slurm_profile, slurm_dir, limit):

    import os
    from astra import config, log
    from astra.utils import expand_path
    resolved_task = "astra.products.mwm.create_all_mwm_products"

    slurm_profile_config = config.get("slurm", dict(profiles={})).get("profiles", {})
    if slurm_profile is not None:
        if slurm_profile not in slurm_profile_config:
            raise click.BadArgumentUsage(f"Cannot find Slurm profile '{slurm_profile}' in Astra config.")            
    else:     
        try_slurm_profile_names = (resolved_task, resolved_task.split(".")[-1], "default")
        for slurm_profile in try_slurm_profile_names:
            if slurm_profile in slurm_profile_config:
                log.info(f"Using Slurm profile '{slurm_profile}'")
                break
        else:
            raise click.BadOptionUsage(f"Cannot find any Slurm profile in Astra config. Use `--slurm-profile PROFILE` to specify. Tried: {', '.join(slurm_profile_config)}")

    if slurm_dir is None:
        from datetime import datetime
        from tempfile import mkdtemp
        prefix = f"{datetime.now().strftime('%Y-%m-%d')}-{resolved_task.split('.')[-1][:30]}-"
        slurm_dir = mkdtemp(prefix=prefix, dir=expand_path(f"$PBS/"))
        os.chmod(slurm_dir, 0o755)
        log.info(f"Using Slurm directory: {slurm_dir}")    
        job_name = f"{os.path.basename(slurm_dir)}"
    else:
        os.makedirs(slurm_dir, exist_ok=True)
        job_name = f"{resolved_task.split('.')[-1]}"

    if limit is None:
        from astra.models import Source
        from astra.products.mwm_summary import DEFAULT_MWM_WHERE

        limit = (
            Source
            .select()
            .where(
                Source.sdss_id.is_null(False)
            &   DEFAULT_MWM_WHERE
            )
        ).count()

    n_proc = 32
    items_per_page = limit // n_proc + 1
    commands = []
    for page in range(1, n_proc + 1):
        commands.append(f"astra execute astra.products.mwm.create_all_mwm_products --page {page} --limit {items_per_page} &")
    commands.append("wait")

    import sys
    if slurm_dir is None:
        from datetime import datetime
        from tempfile import mkdtemp
        prefix = f"{datetime.now().strftime('%Y-%m-%d')}-{resolved_task.split('.')[-1][:30]}-"
        if page:
            prefix += f"{page}-"
        slurm_dir = mkdtemp(prefix=prefix, dir=expand_path(f"$PBS/"))
        os.chmod(slurm_dir, 0o755)
        log.info(f"Using Slurm directory: {slurm_dir}")    
        job_name = f"{os.path.basename(slurm_dir)}"
    else:
        os.makedirs(slurm_dir, exist_ok=True)
        job_name = f"{resolved_task.split('.')[-1]}"

    slurm_kwds = slurm_profile_config[slurm_profile]

    from astra.utils.slurm import SlurmTask, SlurmJob

    python_threads = slurm_kwds.pop("python_threads", 8)
    if slurm_kwds is None:
        pre_execute_commands = []
    else:
        pre_execute_commands = [
            f"export OMP_NUM_THREADS={python_threads}",
            f"export OPENBLAS_NUM_THREADS={python_threads}",
            f"export MKL_NUM_THREADS={python_threads}",
            f"export VECLIB_MAXIMUM_THREADS={python_threads}",
            f"export NUMEXPR_NUM_THREADS={python_threads}"            
        ]
    slurm_job = SlurmJob(
        [
            SlurmTask(pre_execute_commands + commands)
        ],
        job_name,
        dir=slurm_dir,
        **slurm_kwds,
    )
    slurm_job_pk = slurm_job.submit()

    click.echo(f"{slurm_job_pk}")
    sys.exit(0)    


@cli.command(context_settings=dict(ignore_unknown_options=True))
@click.option("--slurm", is_flag=True, default=False, help="Execute through Slurm (see --slurm-profile)")
@click.option("--slurm-profile", default=None, help="Use Slurm profile specified in Astra config file. If None is given, it will default to the profile for the task name, or `default`.")
@click.option("--slurm-dir", default=None)
@click.option("--page", default=None, type=int)
@click.option("--limit", default=None, type=int)
@click.option("--kwargs-path")
@click.argument("task")
@click.argument("spectra", nargs=-1)
def execute(slurm, slurm_profile, slurm_dir, page, limit, kwargs_path, task, spectra):
    """
    Execute a task on one or many spectra.
    """
    # Resolve spectrum ids.
    #if len(spectra) == 0:
    #    raise click.UsageError("No spectral model or spectrum identifiers given.")

    import os
    import sys
    from astra import config
    from astra.utils import expand_path, log, callable

    import pickle
    from inspect import getfullargspec
    from tqdm import tqdm
    from peewee import chunked, JOIN

    from astra import models
    from astra.models.source import Source
    from astra.models.spectrum import Spectrum, SpectrumMixin

    kwargs = {}
    if kwargs_path is not None:
        with open(kwargs_path, "rb") as fp:
            kwargs = pickle.load(fp)

    # Parse any additional keyword arguments which unfortunately get lumped into `spectra`.
    # TODO: THere must be a nicer way to parse this using click.
    _spectra = []
    for arg in spectra:
        if arg.startswith("--"):
            k, *v = arg[2:].split("=")
            k = k.replace("-", "_")
            kwargs[k] = "=".join(v).strip('"')
        else:
            _spectra.append(arg)

    
    

    # Do some cleverness about the task name.
    for prefix in ("", "astra.", "astra.pipelines.", f"astra.pipelines.{task}."):
        try:
            resolved_task = f"{prefix}{task}"
            f = callable(resolved_task)
        except:
            None
        else:
            if prefix:
                log.info(f"Resolved '{task}' -> '{resolved_task}'")
            break
    else:
        # Raise exception on the no-prefix case.
        f = callable(task)

    # TODO: This is all a bit of spaghetti code. Refactor

    if slurm:
        # Check that there is any work to do before submitting a job.

        try:
            spectrum_pks = list(map(int, _spectra))
        except ValueError:
            if len(_spectra) > 1:
                raise NotImplementedError("Only one spectrum model allowed for now. This can be changed.")
            
            # If the first item has a default, then don't do anything special.
            model_name, = _spectra
            spectrum_model = getattr(models, model_name)
            try:
                output_model = getfullargspec(f).annotations["return"].__args__[0]
            except:
                raise a
                raise ValueError(f"Cannot infer output model for task {f}, is it missing a type annotation?")

            # Query for spectra that does not have a result in this output model
            iterable = (
                spectrum_model
                .select(
                    spectrum_model,
                    Source
                )
                .join(
                    output_model,
                    JOIN.LEFT_OUTER,
                    on=(spectrum_model.spectrum_pk == output_model.spectrum_pk)
                )
                .switch(spectrum_model)
                .join(Source, attr="source") # convenience to pre-fetch .source attribute on everything
                .where(output_model.spectrum_pk.is_null())
                .limit(limit)
            )
            total = limit or iterable.count()
            log.info(f"Found at least {total} {model_name} spectra that do not have results in {output_model}")

        else:
            total = len(spectrum_pks)

        argspec = getfullargspec(f)

        if len(argspec.defaults) != len(argspec.args) and total == 0:
            # Nothing to do.
            log.info(f"No spectra to process.")
            sys.exit(0)
    

        from astra.utils.slurm import SlurmTask, SlurmJob

        # Resolve slurm profile.
        slurm_profile_config = config.get("slurm", dict(profiles={})).get("profiles", {})
        if slurm_profile is not None:
            if slurm_profile not in slurm_profile_config:
                raise click.BadArgumentUsage(f"Cannot find Slurm profile '{slurm_profile}' in Astra config.")            
        else:     
            try_slurm_profile_names = (resolved_task, resolved_task.split(".")[-1], "default")
            for slurm_profile in try_slurm_profile_names:
                if slurm_profile in slurm_profile_config:
                    log.info(f"Using Slurm profile '{slurm_profile}'")
                    break
            else:
                raise click.BadOptionUsage(f"Cannot find any Slurm profile in Astra config. Use `--slurm-profile PROFILE` to specify. Tried: {', '.join(slurm_profile_config)}")
            
        slurm_kwds = slurm_profile_config[slurm_profile]

        # Submit this job. #TODO: Is there a way for Click to reconstruct the command for us?
        command = "astra execute "
        if page:
            command += f"--page {page} "
        if limit:
            command += f"--limit {limit} "
        if kwargs_path:
            command += f"--kwargs-path {kwargs_path} "
        command += f"{resolved_task} "
        command += " ".join(spectra)

        if slurm_dir is None:
            from datetime import datetime
            from tempfile import mkdtemp
            prefix = f"{datetime.now().strftime('%Y-%m-%d')}-{resolved_task.split('.')[-1][:30]}-"
            if page:
                prefix += f"{page}-"
            slurm_dir = mkdtemp(prefix=prefix, dir=expand_path(f"$PBS/"))
            os.chmod(slurm_dir, 0o755)
            log.info(f"Using Slurm directory: {slurm_dir}")    
            job_name = f"{os.path.basename(slurm_dir)}"
        else:
            os.makedirs(slurm_dir, exist_ok=True)
            job_name = f"{resolved_task.split('.')[-1]}"

        python_threads = slurm_kwds.pop("python_threads", 8)
        if slurm_kwds is None:
            pre_execute_commands = []
        else:
            pre_execute_commands = [
                f"export OMP_NUM_THREADS={python_threads}",
                f"export OPENBLAS_NUM_THREADS={python_threads}",
                f"export MKL_NUM_THREADS={python_threads}",
                f"export VECLIB_MAXIMUM_THREADS={python_threads}",
                f"export NUMEXPR_NUM_THREADS={python_threads}"            
            ]
        slurm_job = SlurmJob(
            [
                SlurmTask(pre_execute_commands + [command])
            ],
            job_name,
            dir=slurm_dir,
            **slurm_kwds,
        )
        slurm_job_pk = slurm_job.submit()

        click.echo(f"{slurm_job_pk}")
        sys.exit(0)

    try:
        spectrum_pks = list(map(int, _spectra))
    except ValueError:
        if len(_spectra) > 1:
            raise NotImplementedError("Only one spectrum model allowed for now. This can be changed.")
        
        model_name, = _spectra
        spectrum_model = getattr(models, model_name)
        argspec = getfullargspec(f)

        try:
            output_model = argspec.annotations["return"].__args__[0]
        except:
            raise ValueError(f"Cannot infer output model for task {f}, is it missing a type annotation?")

        # Query for spectra that does not have a result in this output model
        iterable = (
            spectrum_model
            .select(
                spectrum_model,
                Source
            )
            .join(
                output_model,
                JOIN.LEFT_OUTER,
                on=(spectrum_model.spectrum_pk == output_model.spectrum_pk)
            )
            .switch(spectrum_model)
            .join(Source, attr="source") # convenience to pre-fetch .source attribute on everything
            .where(output_model.spectrum_pk.is_null())
        )
        if page:
            iterable = (
                iterable
                .paginate(page, limit)
            )
        else:
            iterable = iterable.limit(limit)
        total = limit or iterable.count()
    
    else:            
        if not spectrum_pks:
            spectrum_pks.extend(kwargs.pop("spectra", []))
        else:
            if "spectra" in kwargs:
                raise ValueError("`spectra` given in `kwargs_path` and in command line")
    
        argspec = getfullargspec(f)

        if len(argspec.defaults) == len(argspec.args):
            # It has a default for everything, and no spectrum model given, so give nothing
            iterable = None
        elif spectrum_pks:                
            example = Spectrum.get(spectrum_pks[0])
            spectrum_model = None
            for expr, field in example.dependencies():
                if SpectrumMixin not in field.model.__mro__:
                    continue
                try:
                    q = list(field.model.select().where(expr))
                except:
                    continue
                else:
                    if q:
                        spectrum_model = field.model
                        log.info(f"Identified input spectra as type `{spectrum_model}`")
                        break
            
            log.warning(f"All given spectrum identifiers should come from the same model type")

            # SQLite has a limit on how many SQL variables can be used in a transaction.
            def yield_spectrum_chunks():
                for chunk in chunked(spectrum_pks, 10_000):
                    yield from (
                        spectrum_model
                        .select(
                            spectrum_model,
                            Source
                        )
                        .join(Source, attr="source")
                        .where(spectrum_model.spectrum_pk.in_(chunk))
                    )

            iterable = yield_spectrum_chunks()
            total = len(spectrum_pks)
        else:
            raise click.UsageError("Could not resolve spectrum identifiers.")

    if page is not None:
        kwargs["page"] = page
    if limit is not None:
        kwargs["limit"] = limit
        
    if iterable is None:
        for result in tqdm(f(**kwargs), total=0, unit=" spectra"):
            None
    
    else:
        for result in tqdm(f(iterable, **kwargs), total=total, unit=" spectra"):
            None
    
    return None



@cli.command()
@click.argument("paths", nargs=-1)
def run(paths, **kwargs):
    """Execute one or many tasks."""
    import os
    import json
    from importlib import import_module
    from astra.utils import log, expand_path
    from astra.database.astradb import DataProduct
    from tqdm import tqdm

    for path in paths:
        log.info(f"Running {path}")
        with open(path, "r") as fp:
            content = json.load(fp)
         
        instructions = [content] if isinstance(content, dict) else content
        N = len(instructions)
        for i, instruction in enumerate(instructions, start=1):
            log.info(f"Starting on instruction {i}/{N} in {path}")

            task_kwargs = instruction.get("task_kwargs", {})

            # A couple of unfortunate hacks to fix instructions that were incomplete.
            if (instruction["task_callable"] == "astra.contrib.aspcap.abundances.aspcap_abundances"):
                if "pwd" not in task_kwargs:
                    # This one will fail.
                    log.warning(f"Skipping {i}-th (1-indexed) instruction because it's ASPCAP abundances without a pwd")
                    continue
                
                # Check if outputs already exist.
                pwd = task_kwargs["pwd"]
                if os.path.exists(os.path.join(expand_path(pwd), "stdout")):
                    log.warning(F"Skipping {i}-th (1-indexed) instruction because it's ASPCAP abundances and the outputs already exist")
                    continue            

            # Get the task executable.
            module_name, function_name = instruction["task_callable"].rsplit(".", 1)
            module = import_module(module_name)
            task_callable = getattr(module, function_name)

            has_data_products = "data_product" in task_kwargs # TODO: this special key should be defined elsewhere
            if has_data_products:
                # Resolve the data products
                input_data_products = task_kwargs.pop("data_product", [])
                if isinstance(input_data_products, str):
                    input_data_products = json.loads(input_data_products)
                # The same data product can appear in this list multiple times, and we want to preserve order.
                q = DataProduct.select().where(DataProduct.id << input_data_products)
                unique_data_products = { dp.id: dp for dp in q }
                task_kwargs["data_product"] = [unique_data_products[dp_pk] for dp_pk in input_data_products]
                
            log.info(f"Executing..")
            try:
                results = task_callable(**task_kwargs)
                for result in results:
                    None
            except:
                log.exception(f"Exception in {task_callable} with {task_kwargs}")
                raise
                continue


    log.info(f"Done")
    
    # Remove the path now that we're done.
    #try:
    #    os.unlink(path)
    #except:
    #    None



if __name__ == "__main__":
    cli(obj=dict())
